{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56717,"status":"ok","timestamp":1651798125944,"user":{"displayName":"Jian Mao","userId":"18024972620492169764"},"user_tz":240},"id":"_Wlp23vvm02X","outputId":"e40f41b5-7e46-4bec-94dc-f5d9d5b520ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 32 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.3\n","  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n","\u001b[K     |████████████████████████████████| 198 kB 44.8 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=084015c465d86546b23ecfbb0ffb2c557ae9e18aaa3680941beccbc16351587c\n","  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"]}],"source":["!pip install pyspark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5507,"status":"ok","timestamp":1651798146135,"user":{"displayName":"Jian Mao","userId":"18024972620492169764"},"user_tz":240},"id":"jXsTB0vmcyQD","outputId":"27154cb4-689e-4f07-8f37-5baa4409233e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting twarc\n","  Downloading twarc-2.10.4-py3-none-any.whl (59 kB)\n","\u001b[?25l\r\u001b[K     |█████▌                          | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 30 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 59 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8 in /usr/local/lib/python3.7/dist-packages (from twarc) (2.8.2)\n","Requirement already satisfied: tqdm>=4.62 in /usr/local/lib/python3.7/dist-packages (from twarc) (4.64.0)\n","Collecting click-config-file>=0.6\n","  Downloading click_config_file-0.6.0-py2.py3-none-any.whl (6.0 kB)\n","Requirement already satisfied: requests-oauthlib>=1.3 in /usr/local/lib/python3.7/dist-packages (from twarc) (1.3.1)\n","Collecting humanize>=3.9\n","  Downloading humanize-4.1.0-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: click<9,>=7 in /usr/local/lib/python3.7/dist-packages (from twarc) (7.1.2)\n","Collecting click-plugins>=1\n","  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n","Collecting configobj>=5.0.6\n","  Downloading configobj-5.0.6.tar.gz (33 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from configobj>=5.0.6->click-config-file>=0.6->twarc) (1.15.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from humanize>=3.9->twarc) (4.11.3)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=1.3->twarc) (3.2.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=1.3->twarc) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->requests-oauthlib>=1.3->twarc) (2021.10.8)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->humanize>=3.9->twarc) (4.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->humanize>=3.9->twarc) (3.8.0)\n","Building wheels for collected packages: configobj\n","  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for configobj: filename=configobj-5.0.6-py3-none-any.whl size=34547 sha256=7c29e2d53e67e01b52e2a465235487456c0956330de0d858ce90b72e0faa3ee9\n","  Stored in directory: /root/.cache/pip/wheels/0d/c4/19/13d74440f2a571841db6b6e0a273694327498884dafb9cf978\n","Successfully built configobj\n","Installing collected packages: configobj, humanize, click-plugins, click-config-file, twarc\n","  Attempting uninstall: humanize\n","    Found existing installation: humanize 0.5.1\n","    Uninstalling humanize-0.5.1:\n","      Successfully uninstalled humanize-0.5.1\n","Successfully installed click-config-file-0.6.0 click-plugins-1.1.1 configobj-5.0.6 humanize-4.1.0 twarc-2.10.4\n"]}],"source":["pip install twarc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3502,"status":"ok","timestamp":1651798174238,"user":{"displayName":"Jian Mao","userId":"18024972620492169764"},"user_tz":240},"id":"jZjq6xZOc1U2","outputId":"b3cf3e97-71e3-43cb-d689-f39940245145"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting cleantext\n","  Downloading cleantext-1.1.4-py3-none-any.whl (4.9 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from cleantext) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->cleantext) (1.15.0)\n","Installing collected packages: cleantext\n","Successfully installed cleantext-1.1.4\n"]}],"source":["pip install cleantext"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AQ_smZJ8c5aX","executionInfo":{"status":"ok","timestamp":1651798186924,"user_tz":240,"elapsed":10301,"user":{"displayName":"Jian Mao","userId":"18024972620492169764"}},"outputId":"e98ca42a-a1f4-4d51-b042-b638888aa968"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 5.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=ed30475a03e5e7eb0d5dc44c10f6c8bc24bedb7d66e560715c38663ab34faca3\n","  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-1.7.0\n"]}],"source":["pip install emoji"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"khSVCAQtakPz"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","import pyspark.sql.functions as F\n","import pyspark.sql.types as T\n","from twarc import Twarc2, expansions\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from cleantext import clean\n","import emoji\n","from datetime import timezone, timedelta, datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21971,"status":"ok","timestamp":1651798228084,"user":{"displayName":"Jian Mao","userId":"18024972620492169764"},"user_tz":240},"id":"rogmjhXo5L3o","outputId":"38594024-6fd7-4429-f381-7ac640bb7d22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":874,"status":"ok","timestamp":1651798486029,"user":{"displayName":"Jian Mao","userId":"18024972620492169764"},"user_tz":240},"id":"x9rCMPbealK_","outputId":"fd698e3a-62ea-4ac2-b616-5dd92573babd"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}],"source":["nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","metadata":{"id":"U_IDX9RRdxtE"},"source":["# Twitter data collection and cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8SzsyeNbACu"},"outputs":[],"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"test\").master(\"local[*]\").getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"17zTMpjnfJbg"},"source":["### Collecting twitter data using twitter API"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qRFPKiuXa4Kk"},"outputs":[],"source":["# Replace with your twitter api bearer_token\n","bearer_token = \"YOUR_TOKEN\"\n","client = Twarc2(bearer_token=bearer_token)\n","# Specify the start time in UTC for the time period you want replies from\n","start_time = datetime(2022, 5, 2, 0, 0, 0, 0, timezone(timedelta(hours=-5)))\n","# Specify the end time in UTC for the time period you want Tweets from\n","end_time = datetime(2022, 5, 2, 23, 59, 59, 0, timezone(timedelta(hours=-5)))\n","# Use stock ticker symbol as the key word to specify the query \n","query = \"\\$AMZN -is:retweet lang:en\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmAOEEgba8Pl"},"outputs":[],"source":["# Search the tweets\n","search_results = client.search_recent(query=query, start_time=start_time, end_time=end_time, tweet_fields=[\"created_at\"])\n","\n","# Put the search results in a spark dataframe\n","data = []\n","for page in search_results:\n","  # get all the information in a single JSON\n","  tweets = expansions.flatten(page)\n","  for tweet in tweets:\n","    time_str = tweet['created_at'][:10]\n","    data.append([datetime.fromisoformat(time_str).strftime(\"%d-%m-%Y\"), tweet[\"text\"]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CqnmWDk4scX"},"outputs":[],"source":["# save the searching result in a spark dataframe\n","tweets_df = spark.createDataFrame(data=data, schema=[\"time\", \"text\"])\n","tweets_df.head(10)"]},{"cell_type":"markdown","source":["### Tweets cleaning"],"metadata":{"id":"OBSgYcjY_HmU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZowZGAaFbEKY"},"outputs":[],"source":["# remove links\n","tweets_df_cleaned = tweets_df.withColumn('text', F.regexp_replace('text', r\"http\\S+\", ''))\n","tweets_df_cleaned = tweets_df_cleaned.withColumn('text', F.regexp_replace('text', r\"www.\\S+\", ''))\n","\n","# remove mentions\n","tweets_df_cleaned = tweets_df_cleaned.withColumn('text', F.regexp_replace('text', \"@\\s*[A-Za-z0-9_]+\", ''))\n","\n","# remove emoji\n","def remove_emoji(col):\n","  result = emoji.demojize(col)\n","  return result\n","\n","# lower letters, remove extra blank space and punctuation\n","def lower_and_no_extra_space(col):\n","    if col != \"\":\n","        result = clean(col, lowercase=True, extra_spaces=True, punct=True)\n","        return result\n","    else:\n","        return col\n","\n","# remove stopwords\n","# Loading stop words and removing negative stop words from the list\n","stop_words = stopwords.words('english')\n","words_to_keep = [\"don\", \"don’t\", \"ain\", \"aren\", \"aren’t\", \"couldn\", \"couldn’t\", \"didn\", \"didn’t\", \"doesn\", \"doesn’t\",\n","                 \"hadn\", \"hadn’t\", \"hasn\", \"hasn’t\", \"haven\", \"haven’t\", \"isn\", \"isn’t\", \"ma\", \"mightn\", \"mightn’t\",\n","                 \"mustn’t\", \"needn\", \"needn’t\", \"shan\", \"shan’t\", \"no\", \"nor\", \"not\", \"shouldn\", \"shouldn’t\", \"wasn\",\n","                 \"wasn’t\", \"weren\", \"weren’t\", \"won\", \"won’t\", \"wouldn\", \"wouldn’t\"]\n","my_stop_words = stop_words\n","for word in words_to_keep:\n","    if word in my_stop_words:\n","        my_stop_words.remove(word)\n","\n","# Removing stop words from the tweet\n","def remove_stop_words(col):\n","  tokens = word_tokenize(col)\n","  tweet_with_no_stop_words = [token for token in tokens if token not in my_stop_words]\n","  reformed_tweet = ' '.join(tweet_with_no_stop_words)\n","  return reformed_tweet\n","\n","# apply the UDF to remove emoji\n","clean_udf = F.UserDefinedFunction(remove_emoji, T.StringType())\n","tweets_df_cleaned = tweets_df_cleaned.withColumn('text', clean_udf('text'))\n","\n","# remove stopwords\n","stopwords_udf = F.UserDefinedFunction(remove_stop_words, T.StringType())\n","tweets_df_cleaned = tweets_df_cleaned.withColumn('text', stopwords_udf('text'))\n","\n","# convert to lowercase, remove extra blank space and some punctuation\n","lower = F.UserDefinedFunction(lower_and_no_extra_space, T.StringType())\n","tweets_df_cleaned = tweets_df_cleaned.withColumn('text', lower('text'))\n","\n","# remove remaining punctuation\n","tweets_df_cleaned = tweets_df_cleaned.withColumn('text', F.regexp_replace('text', r'[^\\w\\s]', ' '))\n","\n","# remove remaining blank space\n","tweets_df_cleaned = tweets_df_cleaned.withColumn('text', F.regexp_replace('text', r'\\s+', ' '))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H8XmimqDH7wW"},"outputs":[],"source":["tweets_df_cleaned.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t7KYzxYa4r-_"},"outputs":[],"source":["# save the cleaned data in a csv file\n","pd_df = tweets_df_cleaned.toPandas()\n","pd_df.to_csv(path_or_buf=\"/AMZN.csv\", sep=';', index=False, encoding='utf-8', mode='a+')"]},{"cell_type":"markdown","source":["# Sentiment Analysis"],"metadata":{"id":"Eery-Mm9-3lV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"01UBKllY-dko"},"outputs":[],"source":["#################################################################################\n","## sentiment analysis model training \n","#################################################################################\n","\n","from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n","from pyspark.ml.feature import StringIndexer, VectorIndexer, IndexToString\n","from pyspark.ml import Pipeline\n","\n","from pyspark.ml.classification import LinearSVC\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.classification import NaiveBayes, GBTClassifier\n","\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n","\n","\n","train_data = spark.read.csv(\"/sen140_1w.csv\",header='true', inferSchema='true')\n","# train_data.show(5)\n","(train_set, val_set, test_set) = train_data.randomSplit([0.9, 0.05, 0.05], seed = 123)\n","\n","tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n","cv = CountVectorizer(vocabSize=2**16, inputCol=\"words\", outputCol='cv')\n","idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5)\n","label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n","lr = LogisticRegression(maxIter=50, regParam=0.8, elasticNetParam=0.01)\n","pipeline = Pipeline(stages=[tokenizer, cv, idf, label_stringIdx, lr])\n","\n","pipelineFit = pipeline.fit(train_set)\n","predictions = pipelineFit.transform(val_set)\n","# predictions.select(\"text\", \"target\", \"label\", \"prediction\").show(5)\n","print(predictions.show(5))\n","\n","evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n","accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val_set.count())\n","roc_auc = evaluator.evaluate(predictions)\n","\n","print(\"Accuracy Score: {0:.4f}\".format(accuracy))\n","print(\"ROC-AUC: {0:.4f}\".format(roc_auc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5cAj4I_-8Ig"},"outputs":[],"source":["#################################################################################\n","## new data sentiment analysis scores prediction\n","#################################################################################\n","\n","df_text = tweets_df_cleaned.na.fill(\"\")\n","\n","# predict sentiment analysis scores\n","text_predictions = pipelineFit.transform(df_text)\n","# predictions.select(\"text\", \"target\", \"label\", \"prediction\").show(5)\n","print(text_predictions.show(10))\n","print(\"Text scores prediction succeeded.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPUvIkWq_JE1"},"outputs":[],"source":["# save prediction results to csv\n","df_scores = text_predictions.select(\"time\",\"text\",\"prediction\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T2xWqOw6_MfT"},"outputs":[],"source":["# average scores by date\n","# normalize scores considering count numbers\n","\n","date_scores = df_scores.groupBy(\"time\").agg(F.mean('prediction').alias(\"mean\"),F.count('prediction').alias(\"count\"))\n","normalized_scores = date_scores.withColumn(\"normalized\", ( F.col(\"mean\") / F.exp(1/F.col(\"count\")) ) )\n","normalized_scores.show(5)\n","print(\"Average scores by date and normalization succeeded.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FbNLM9ZCs9W"},"outputs":[],"source":["# save the normalized score in a csv file\n","pd_df = normalized_scores.toPandas()\n","pd_df.to_csv(path_or_buf=\"/score.csv\", sep=';', index=False, encoding='utf-8', mode='a+')"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Tweets collection and sentiment analysis.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}